{"cells":[{"cell_type":"markdown","id":"64ba026a","metadata":{"id":"64ba026a"},"source":["## 0. Installation and Setup"]},{"cell_type":"code","execution_count":null,"id":"5062bfde","metadata":{"scrolled":true,"id":"5062bfde"},"outputs":[],"source":["! pip install langchain"]},{"cell_type":"markdown","id":"d8a900f3","metadata":{"id":"d8a900f3"},"source":["## 1. Load Data\n","In Langchiain, we use document_loaders to load our data. We can simply import langchain.document_loaders and specify the data type.\n","1. folder: DirectoryLoader\n","2. Azure: AzureBlobStorageContainerLoader\n","3. CSV file: CSVLoader\n","4. Google Drive: GoogleDriveLoader\n","5. Website: UnstructuredHTMLLoader\n","6. PDF: PyPDFLoader\n","7. Youtube: YoutubeLoader\n","\n","For more data loader refer to the following link:\n","https://python.langchain.com/docs/modules/data_connection/document_loaders.html"]},{"cell_type":"code","execution_count":null,"id":"4acd29a3","metadata":{"id":"4acd29a3"},"outputs":[],"source":["# take pdf as a exapmle. This is helpful if we directly download the documents from company website.\n","\n","from langchain.document_loaders import PyPDFLoader\n","\n","loader = PyPDFLoader(\"example_data/layout-parser-paper.pdf\")\n","pages = loader.load_and_split()\n","\n","\n","# We can also use github (Website type) to store our original data.\n","\n","from langchain.document_loaders import WebBaseLoader\n","\n","loader = WebBaseLoader(\"https://name.github.io/folder_name/document_name\")\n","data = loader.load()"]},{"cell_type":"markdown","id":"77553622","metadata":{"id":"77553622"},"source":["## 2. Split the data\n","Once we loaded documents, we need to transform them to better suit our application. The simplest example is to split a long document into smaller chunks that can fit into our model's context window. The most common Splitter in LangChain includes:\n","\n","1. RecursiveCharacterTextSplitter()\n","2. CharacterTextSplitter()\n","\n","The paramether of above functions:\n"," - length_function: how the length of chunks is calculated. Defaults to just counting number of characters, but it's pretty common to pass a token counter here.\n"," - chunk_size: the maximum size of your chunks (as measured by the length function).\n"," - chunk_overlap: the maximum overlap between chunks. It can be nice to have some overlap to maintain some continuity between chunks (e.g. do a sliding window).\n"," - add_start_index: whether to include the starting position of each chunk within the original document in the metadata."]},{"cell_type":"code","execution_count":null,"id":"17e50f00","metadata":{"id":"17e50f00"},"outputs":[],"source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n","all_splits = text_splitter.split_documents(data)"]},{"cell_type":"markdown","id":"8a7bc213","metadata":{"id":"8a7bc213"},"source":["## 3. Vectorstores\n","Since the input of model is vector instead of character, we need to transfer the text data into vector space(embeddding). There are already some useful vector database like ChromaDB, Milvus, pgvector...\n","\n","Before we load the data into vector database, we need a perfect embeddings model.The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc).\n","\n","https://python.langchain.com/en/latest/modules/indexes/vectorstores.html"]},{"cell_type":"code","execution_count":null,"id":"89d18673","metadata":{"id":"89d18673"},"outputs":[],"source":["from langchain.embeddings import OpenAIEmbeddings\n","from langchain.vectorstores import Chroma\n","\n","# Initialize openaiâ€™s embeddings object\n","embeddings = OpenAIEmbeddings() # api-key needed\n","\n","# Calculate the embedding vector information of the document through the embeddings object of openai and temporarily store it in the Chroma vector database for subsequent matching queries.\n","\n","vectorstore = Chroma.from_documents(documents=all_splits, embeddings)"]},{"cell_type":"markdown","id":"1c35d49c","metadata":{"id":"1c35d49c"},"source":["## 4.Retrive\n","Retrieve relevant splits for any question using similarity search. There are servral way for retrievals, Vectorstores+similarity_search are commonly used. We can also use SVM Retriever."]},{"cell_type":"code","execution_count":null,"id":"a92138fd","metadata":{"id":"a92138fd"},"outputs":[],"source":["question = \"Our answer here\"\n","\n","# Vectorstores+ s imilarity_search\n","docs = vectorstore.similarity_search(question)\n","\n","\n","# SVM Retriever\n","from langchain.retrievers import SVMRetriever\n","\n","svm_retriever = SVMRetriever.from_documents(all_splits,OpenAIEmbeddings())\n","docs_svm=svm_retriever.get_relevant_documents(question)"]},{"cell_type":"markdown","id":"b0c27245","metadata":{"id":"b0c27245"},"source":["## 5. Generate Answer\n","The key function of this part is RetrievalQA(). We need to feed our model, retriever and prompt into the function to create Q&A object.\n","\n","For details on RetrievalQA, refers to\n","https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html"]},{"cell_type":"code","execution_count":null,"id":"ee9fc26a","metadata":{"scrolled":true,"id":"ee9fc26a"},"outputs":[],"source":["from langchain.chains import RetrievalQA\n","from langchain.prompts import PromptTemplate\n","\n","# Use a answer template as the prompt feeded into the model\n","template = \"\"\"Use the following pieces of context to answer the question at the end.\n","If you don't know the answer, just say that you don't know, don't try to make up an answer.\n","Use three sentences maximum and keep the answer as concise as possible.\n","Always say \"thanks for asking!\" at the end of the answer.\n","{context}\n","Question: {question}\n","Helpful Answer:\"\"\"\n","QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n","\n","\n","# load our model\n","llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","# Create Q&A object\n","qa_chain = RetrievalQA.from_chain_type(\n","    llm,\n","    retriever=vectorstore.as_retriever(),\n","    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",")\n","\n","# Feed our question and get the answer.\n","result = qa_chain({\"query\": question})\n","result[\"result\"]\n"]},{"cell_type":"markdown","id":"f4c1ba95","metadata":{"id":"f4c1ba95"},"source":["Reference:\n","https://python.langchain.com/docs/use_cases/question_answering/#step-4-retrieve"]},{"cell_type":"code","execution_count":null,"id":"f4b6fd95","metadata":{"id":"f4b6fd95"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}