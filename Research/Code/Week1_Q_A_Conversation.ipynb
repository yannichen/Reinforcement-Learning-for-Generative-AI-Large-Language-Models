{"cells":[{"cell_type":"markdown","id":"64ba026a","metadata":{"id":"64ba026a"},"source":["## 0. Installation and Setup"]},{"cell_type":"code","source":["# hide output\n","%%capture output\n","\n","! pip install pdfplumber\n","! pip install chromadb\n","! pip install pymilvus\n","! pip install sentence-transformers\n","! pip install langchain\n","! pip install pypdf"],"metadata":{"id":"qjvYxbtVtE1k"},"id":"qjvYxbtVtE1k","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"d8a900f3","metadata":{"id":"d8a900f3"},"source":["## 1. Load Data\n","In Langchiain, we use document_loaders to load our data. We can simply import langchain.document_loaders and specify the data type.\n","1. folder: DirectoryLoader\n","2. Azure: AzureBlobStorageContainerLoader\n","3. CSV file: CSVLoader\n","4. Google Drive: GoogleDriveLoader\n","5. Website: UnstructuredHTMLLoader\n","6. PDF: PyPDFLoader\n","7. Youtube: YoutubeLoader\n","\n","For more data loader refer to the following link:\n","https://python.langchain.com/docs/modules/data_connection/document_loaders.html"]},{"cell_type":"code","source":["import os\n","from google.colab import drive\n","# Access drive\n","drive.mount('/content/drive')\n","path = '/content/drive/MyDrive/Capstone/'\n","\n","\n","# companies\n","companies = os.listdir(os.path.join(path, 'Company Reports'))\n","for i, comp in enumerate(companies):\n","    print(i, \": \", comp)\n","\n","\n","# get reports\n","def get_reports(comp, year):\n","    \"\"\"\n","    comp:   string or index\n","    year:   specific year or # recent year, 0 for all\n","    ret:    list of report pathes\n","    \"\"\"\n","    if type(comp) == str:\n","        if comp not in companies:\n","            print(\"Error: \", comp, \" does not exist\")\n","            return\n","    elif type(comp) == int:\n","        if comp not in range(len(companies)):\n","            print(\"Error: invalid index\")\n","            return\n","        comp = companies[comp]\n","    else:\n","        print(\"Error: invalid company\")\n","        return\n","\n","    if type(year) != int:\n","        print(\"Error: invalid year\")\n","        return\n","\n","    file_path = os.path.join(path, 'Company Reports', comp)\n","    files = os.listdir(file_path)\n","    files.sort(reverse=True)\n","\n","    if year in range(11):\n","        if year:\n","            files = files[:year]\n","    else:\n","        files = [f for f in files if str(year) in f]\n","    return [os.path.join(file_path, file) for file in files]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eZ4c6uvzXbIJ","executionInfo":{"status":"ok","timestamp":1696430221690,"user_tz":240,"elapsed":20981,"user":{"displayName":"Yi Lu","userId":"06586351092226345030"}},"outputId":"21f8e73b-8d3c-42e8-e71c-1e17d447bde7"},"id":"eZ4c6uvzXbIJ","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","0 :  ExxonMobil\n","1 :  Shell plc\n","2 :  BP PLC\n","3 :  Saudi Aramco\n","4 :  Chevron\n","5 :  TotalEnergies\n","6 :  Valero Energy\n","7 :  Marathon Petroleum Corporation\n","8 :  Sinopec\n","9 :  PetroChina\n"]}]},{"cell_type":"code","source":["get_reports(2, 3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zTFtO1HBdkYr","executionInfo":{"status":"ok","timestamp":1696430226755,"user_tz":240,"elapsed":331,"user":{"displayName":"Yi Lu","userId":"06586351092226345030"}},"outputId":"62de9ebb-534e-4b96-c354-8479c36a04ae"},"id":"zTFtO1HBdkYr","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/drive/MyDrive/Capstone/Company Reports/BP PLC/BP PLC_2022.pdf',\n"," '/content/drive/MyDrive/Capstone/Company Reports/BP PLC/BP PLC_2021.pdf',\n"," '/content/drive/MyDrive/Capstone/Company Reports/BP PLC/BP PLC_2020.pdf']"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","execution_count":null,"id":"4acd29a3","metadata":{"id":"4acd29a3","colab":{"base_uri":"https://localhost:8080/","height":383},"executionInfo":{"status":"error","timestamp":1696430228646,"user_tz":240,"elapsed":5,"user":{"displayName":"Yi Lu","userId":"06586351092226345030"}},"outputId":"3a160160-980c-436d-ee54-f7e316544906"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-26f9ad7a48f8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# take pdf as a exapmle. This is helpful if we directly download the documents from company website.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPyPDFLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_reports\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2018\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["# take pdf as a exapmle. This is helpful if we directly download the documents from company website.\n","from langchain.document_loaders import PyPDFLoader\n","\n","file = get_reports(0, 2018)\n","\n","loader = PyPDFLoader(file[0])\n","data = loader.load_and_split()\n","\n","\n","# We can also use github (Website type) to store our original data.\n","\n","# from langchain.document_loaders import WebBaseLoader\n","\n","# loader = WebBaseLoader(\"https://drive.google.com/file/d/1EA8Iifu4kSIfziXAYz33P7Zon_u_beWb/view?usp=drive_link\")\n","# data = loader.load()"]},{"cell_type":"markdown","id":"77553622","metadata":{"id":"77553622"},"source":["## 2. Split the data\n","Once we loaded documents, we need to transform them to better suit our application. The simplest example is to split a long document into smaller chunks that can fit into our model's context window. The most common Splitter in LangChain includes:\n","\n","1. RecursiveCharacterTextSplitter()\n","2. CharacterTextSplitter()\n","\n","The paramether of above functions:\n"," - length_function: how the length of chunks is calculated. Defaults to just counting number of characters, but it's pretty common to pass a token counter here.\n"," - chunk_size: the maximum size of your chunks (as measured by the length function).\n"," - chunk_overlap: the maximum overlap between chunks. It can be nice to have some overlap to maintain some continuity between chunks (e.g. do a sliding window).\n"," - add_start_index: whether to include the starting position of each chunk within the original document in the metadata."]},{"cell_type":"code","execution_count":null,"id":"17e50f00","metadata":{"id":"17e50f00"},"outputs":[],"source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size = 200, chunk_overlap = 0)\n","all_splits = text_splitter.split_documents(data)\n"]},{"cell_type":"markdown","id":"8a7bc213","metadata":{"id":"8a7bc213"},"source":["## 3. Vectorstores\n","Since the input of model is vector instead of character, we need to transfer the text data into vector space(embeddding). There are already some useful vector database like ChromaDB, Milvus, pgvector...\n","\n","Before we load the data into vector database, we need a perfect embeddings model.The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc).\n","\n","https://python.langchain.com/en/latest/modules/indexes/vectorstores.html"]},{"cell_type":"code","source":["from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores import Chroma\n","\n","embeddings = HuggingFaceEmbeddings()\n","\n","\n","comp = companies[0]\n","year = 2018\n","\n","comp_path = os.path.join(path, 'chroma_db', comp + '_' + str(year))\n","\n","# load from document\n","vectorstore = Chroma.from_documents(all_splits, embeddings, persist_directory=comp_path)\n","\n","# load from disk\n","#vectorstore = Chroma(persist_directory=comp_path, embedding_function=embeddings)"],"metadata":{"id":"9_4YxZLkf0yz"},"id":"9_4YxZLkf0yz","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"1c35d49c","metadata":{"id":"1c35d49c"},"source":["## 4.Retrive\n","Retrieve relevant splits for any question using similarity search. There are servral way for retrievals, Vectorstores+similarity_search are commonly used. We can also use SVM Retriever."]},{"cell_type":"code","source":["question = \"What's the upstream earnings after income tax in 2017?\"\n","\n","# Vectorstores+ s imilarity_search\n","docs = vectorstore.similarity_search(question)\n","\n","\n","## Another algo SVM Retriever\n","from langchain.retrievers import SVMRetriever\n","\n","svm_retriever = SVMRetriever.from_documents(all_splits, embeddings)\n","docs_svm=svm_retriever.get_relevant_documents(question)"],"metadata":{"id":"CNrw3NLZtVx6"},"id":"CNrw3NLZtVx6","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. Model\n","The LLM we are using"],"metadata":{"id":"b7Wz9ECbnUkG"},"id":"b7Wz9ECbnUkG"},{"cell_type":"code","source":["from langchain.llms import HuggingFacePipeline\n","from transformers import AutoTokenizer, pipeline, AutoModelForSeq2SeqLM\n","\n","model_id = 'google/flan-t5-large'\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n","\n","pipe = pipeline(\n","    \"text2text-generation\",\n","    model = model,\n","    tokenizer = tokenizer,\n","    max_length = 100\n",")\n","\n","llm = HuggingFacePipeline(pipeline = pipe)"],"metadata":{"id":"rujabhWDnOx-"},"id":"rujabhWDnOx-","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"b0c27245","metadata":{"id":"b0c27245"},"source":["## 6. Generate Answer\n","The key function of this part is RetrievalQA(). We need to feed our model, retriever and prompt into the function to create Q&A object.\n","\n","For details on RetrievalQA, refers to\n","https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html"]},{"cell_type":"code","source":["from langchain.chains.question_answering import load_qa_chain\n","\n","chain = load_qa_chain(llm, chain_type=\"stuff\")\n","chain({\"input_documents\": docs, \"question\": question}, return_only_outputs=True)['output_text']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"ATf-CEwEtfN-","outputId":"47f43507-c69c-4c17-a097-d9568c374e8b","executionInfo":{"status":"ok","timestamp":1696390096570,"user_tz":240,"elapsed":1299,"user":{"displayName":"Yi Lu","userId":"06586351092226345030"}}},"id":"ATf-CEwEtfN-","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'14,079'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["## 7. Q&A Wrapper\n","Retriver wrapped here"],"metadata":{"id":"iuSGgXnluRqr"},"id":"iuSGgXnluRqr"},{"cell_type":"code","source":["# wrapper function\n","def get_answer(question) :\n","  docs = vectorstore.similarity_search(question)\n","  res = chain({\"input_documents\": docs, \"question\": question}, return_only_outputs=True)\n","  return res['output_text']"],"metadata":{"id":"ww6WNXRTu66q"},"id":"ww6WNXRTu66q","execution_count":null,"outputs":[]},{"cell_type":"code","source":["question = \"What's the upstream earnings after income tax in 2017?\"\n","get_answer(question)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"x72v-2iGt0J8","outputId":"504bab5b-e0b9-4cfc-d515-cd9f43360d66"},"id":"x72v-2iGt0J8","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'14,079'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":65}]},{"cell_type":"markdown","source":["## 8. Conversation\n"],"metadata":{"id":"MgcqE9QBfylp"},"id":"MgcqE9QBfylp"},{"cell_type":"code","source":["from langchain.chains import ConversationalRetrievalChain\n","\n","# svm_retriever = SVMRetriever.from_documents(all_splits, embeddings)\n","qa = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever())\n","\n","chat_history = []\n","while True:\n","  question = input('Send a question:')\n","  # Use chat_history to store history data\n","  result = qa({'question': question, 'chat_history': chat_history})\n","  chat_history.append((question, result['answer']))\n","  print(result['answer'])"],"metadata":{"id":"UxC4-Drff1KG","colab":{"base_uri":"https://localhost:8080/","height":633},"executionInfo":{"status":"error","timestamp":1696388017528,"user_tz":240,"elapsed":371486,"user":{"displayName":"Yi Lu","userId":"06586351092226345030"}},"outputId":"38a1773a-5c06-4de2-a239-32dcc43b3811"},"id":"UxC4-Drff1KG","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Send a question:What's the upstream earnings after income tax in 2017?\n","14,079\n","Send a question:What's the upstream earnings after income tax next year?\n","14,079\n","Send a question:What's the upstream earnings before income tax in 2017?\n","14,079\n","Send a question:What's the liquid production in 2017?\n","4.7 million barrels per day\n","Send a question:Does it perform well?\n","Technology has allowed us to effectively respond to a dynami c and challenging underpins our strong market position in high-performance products\n","Send a question:Who is the CEO?\n","Douglas R. Oberhelman\n","Send a question:Wrong answer, find another one\n","(iii).\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-e5f0576905b1>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mchat_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Send a question:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0;31m# Use chat_history to store history data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'chat_history'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchat_history\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]},{"cell_type":"code","source":[],"metadata":{"id":"iEAeEsoiutKR"},"id":"iEAeEsoiutKR","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Pp4ALYvVutHo"},"id":"Pp4ALYvVutHo","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"V3m5qr07u9N3"},"id":"V3m5qr07u9N3","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 8. Old code"],"metadata":{"id":"Hx9xOb7-uubX"},"id":"Hx9xOb7-uubX"},{"cell_type":"code","source":["from langchain.chains.llm import LLMChain\n","from langchain.prompts import PromptTemplate\n","from langchain.llms import HuggingFacePipeline\n","from transformers import AutoTokenizer, pipeline, AutoModelForSeq2SeqLM\n","from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n","from langchain.chains import RetrievalQA\n","from langchain.prompts import PromptTemplate\n","\n","model_id = 'google/flan-t5-large'\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n","\n","pipe = pipeline(\n","    \"text2text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    max_length=100\n",")\n","\n","llm = HuggingFacePipeline(pipeline=pipe)\n","\n","\n","# Create Q&A object\n","template = \"\"\"Use the following pieces of context to answer the question at the end.\n","If you don't know the answer, just say that you don't know, don't try to make up an answer.\n","Use three sentences maximum and keep the answer as concise as possible.\n","Always say \"thanks for asking!\" at the end of the answer.\n","{context}\n","Question: {question}\n","Helpful Answer:\"\"\"\n","QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n","\n","\n","\n","qa_chain = RetrievalQA.from_chain_type(\n","    llm,\n","    retriever = svm_retriever,\n","    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",")\n","\n","# Feed our question and get the answer.\n","result = qa_chain({\"query\": question})\n","result[\"result\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"vhjz9mTSobe7","outputId":"7b00fbd4-0c0c-457a-8675-0b1b362a5408"},"id":"vhjz9mTSobe7","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'14,079 13,355 196 7,101 27,548'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["from langchain.chains.llm import LLMChain\n","from langchain.prompts import PromptTemplate\n","from langchain.llms import HuggingFacePipeline\n","from transformers import AutoTokenizer, pipeline, AutoModelForSeq2SeqLM\n","from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n","from langchain.chains import RetrievalQA\n","from langchain.prompts import PromptTemplate\n","\n","model_id = 'google/flan-t5-large'\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n","\n","pipe = pipeline(\n","    \"text2text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    max_length=100\n",")\n","\n","llm = HuggingFacePipeline(pipeline=pipe)\n","\n","\n","# Create Q&A object\n","template = \"\"\"Use the following pieces of context to answer the question at the end.\n","If you don't know the answer, just say that you don't know, don't try to make up an answer.\n","Use three sentences maximum and keep the answer as concise as possible.\n","Always say \"thanks for asking!\" at the end of the answer.\n","{context}\n","Question: {question}\n","Helpful Answer:\"\"\"\n","QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n","\n","\n","\n","qa_chain = RetrievalQA.from_chain_type(\n","    llm,\n","    retriever=vectorstore.as_retriever(),\n","    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",")\n","\n","# Feed our question and get the answer.\n","result = qa_chain({\"query\": question})\n","result[\"result\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"_Rlfvy5ql87w","outputId":"838caae6-20a0-49ab-d71d-90ac8aecc33c"},"id":"_Rlfvy5ql87w","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'14,079 13,355 196 7,101 27,548'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":[],"metadata":{"id":"6j-4iPLInRQv"},"id":"6j-4iPLInRQv","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"f4c1ba95","metadata":{"id":"f4c1ba95"},"source":["Reference:\n","https://python.langchain.com/docs/use_cases/question_answering/#step-4-retrieve"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}